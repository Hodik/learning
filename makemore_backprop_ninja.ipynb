{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hodik/learning/blob/main/makemore_backprop_ninja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sFElPqq8PPp"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "x6GhEWW18aCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f060ad-92e2-4055-bc58-a132c7a0f7db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-01 22:08:58--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-02-01 22:08:58 (8.10 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "klmu3ZG08PPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a43e54-54db-40cc-aa26-1f29db75fe5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BCQomLE_8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a788ccc7-4aa0-490f-a4da-61c6f95356c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V_zt2QHr8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60012f59-b8f3-4af1-a06a-fbe90a64a9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZlFLjQyT8PPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5149c9-1650-470f-bfae-429d75cf44e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ofj1s6d8PPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89f217e-363e-4020-97bc-cb2a3040319f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3345, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embcat.view(n, block_size, n_embd).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD92JOLUydy7",
        "outputId": "eb5a01ea-5a17-44ce-ab56-2792745ecbe0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mO-8aqxK8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2340645e-690c-4faa-ee80-ef70a7fa00a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 4.3655745685100555e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "# -----------------\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = - 1.0 / n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts_sum = -(1.0/counts_sum**2) * dcounts_sum_inv\n",
        "dcounts = counts_sum_inv * dprobs + dcounts_sum\n",
        "dnorm_logits = norm_logits.exp() * dcounts\n",
        "dlogit_maxes = -(dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits = torch.zeros_like(logits)\n",
        "dlogits[range(n), logits.max(1).indices.view(n)] = 1.0\n",
        "dlogits *= dlogit_maxes\n",
        "dlogits += dnorm_logits\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbnraw = dhpreact * bngain\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = 1/(n-1) * dbnvar\n",
        "dbndiff = 2 * bndiff * dbndiff2 + bnvar_inv * dbnraw\n",
        "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
        "dhprebn = 1/n * (torch.ones_like(hprebn) * dbnmeani) + dbndiff\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ebLtYji_8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8801ce-2f92-48aa-b747-b1ab0394ecec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.334463119506836 diff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-gCXbB4C8PPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d58f46-87db-4dfa-b80c-2e450c6b04f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 4.423782229423523e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogits = F.softmax(logits, 1) # TODO. my solution is 3 lines\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hd-MkhB68PPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0c7179-a7b1-4551-fb64-625ce9d74d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "POdeZSKT8PPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae5565a-3f0c-463b-9ec8-525cdf88e16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wPy8DhqB8PPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb13add-1871-43cd-b18c-93c4742cbc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7686\n",
            "  10000/ 200000: 2.1847\n",
            "  20000/ 200000: 2.3868\n",
            "  30000/ 200000: 2.4736\n",
            "  40000/ 200000: 1.9890\n",
            "  50000/ 200000: 2.3194\n",
            "  60000/ 200000: 2.4369\n",
            "  70000/ 200000: 2.0882\n",
            "  80000/ 200000: 2.3467\n",
            "  90000/ 200000: 2.1818\n",
            " 100000/ 200000: 1.9738\n",
            " 110000/ 200000: 2.2480\n",
            " 120000/ 200000: 1.9948\n",
            " 130000/ 200000: 2.4504\n",
            " 140000/ 200000: 2.3773\n",
            " 150000/ 200000: 2.1156\n",
            " 160000/ 200000: 1.9325\n",
            " 170000/ 200000: 1.7549\n",
            " 180000/ 200000: 2.0842\n",
            " 190000/ 200000: 1.8277\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "#with torch.no_grad():\n",
        "\n",
        "# kick off optimization\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmean = hprebn.mean(0, keepdim=True)\n",
        "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "  hpreact = bngain * bnraw + bnbias\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  dlogits = F.softmax(logits, 1) # TODO. my solution is 3 lines\n",
        "  dlogits[range(n), Yb] -= 1\n",
        "  dlogits /= n\n",
        "\n",
        "  dh = dlogits @ W2.T\n",
        "  dW2 = h.T @ dlogits\n",
        "  db2 = dlogits.sum(0)\n",
        "\n",
        "  dhpreact = (1.0 - h**2) * dh\n",
        "\n",
        "  dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "  dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "  dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "  dembcat = dhprebn @ W1.T\n",
        "  dW1 = embcat.T @ dhprebn\n",
        "  db1 = dhprebn.sum(0)\n",
        "\n",
        "  demb = dembcat.view(emb.shape)\n",
        "  dC = torch.zeros_like(C)\n",
        "  for k in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "      ix = Xb[k,j]\n",
        "      dC[ix] += demb[k,j]\n",
        "\n",
        "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "  # -----------------\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p, grad in zip(parameters, grads):\n",
        "    #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "    p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZEpI0hMW8PPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1727c69-b7e4-407a-92b7-8146c1ec39fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.30385160446167e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n",
            "(200,)          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n"
          ]
        }
      ],
      "source": [
        "# useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(range(max_steps)), lossi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Q6WTdl3f0R4a",
        "outputId": "21a9fc3e-91bc-4fbc-b084-a46563a3ca1f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c38e469f7c0>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6aFnP_Zc8PP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94cf658-309f-45df-d2c7-be49116783d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.1019110679626465\n",
            "val 2.1376988887786865\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHeQNv3s8PP1"
      },
      "outputs": [],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}